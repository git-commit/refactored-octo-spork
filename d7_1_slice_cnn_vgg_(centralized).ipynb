{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "d7.1 slice cnn vgg (centralized)",
      "provenance": [],
      "collapsed_sections": [
        "fzrISIoRnBaw"
      ],
      "authorship_tag": "ABX9TyMvMsmspvzJfu9bNbuJUsn4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-commit/refactored-octo-spork/blob/main/d7_1_slice_cnn_vgg_(centralized).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihL-Q_WeycsB"
      },
      "source": [
        "# Centralized Learning experiment with D7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ypjjMiG-Kw3"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1do515arL3xF"
      },
      "source": [
        "agent_name = \"CHANGEME\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEYNQKVU370g"
      },
      "source": [
        "project_name = \"Thesis d7.1 (development)\"\n",
        "num_clients = 1\n",
        "\n",
        "config_default = {\"num_clients\": num_clients,\n",
        "                  \"central_dataset_index\": 0,\n",
        "        \"client_lr\":0.0001,\n",
        "        \"lr_decay\": 0.05,\n",
        "        \"epochs\": 150,\n",
        "        \"batch_size\":64,\n",
        "        \"loss_function\": \"categorical_crossentropy\",\n",
        "        \"model\": \"VGG_Simple\",\n",
        "        \"dataset\": \"HUST-19 (slices)\",\n",
        "        \"dataset_size_limit\": None,\n",
        "        \"sampling\": None if num_clients < 2 else \"stratified_shuffle_deterministic\",\n",
        "        \"random_seed\": 4242\n",
        "      }\n",
        "\n",
        "debug_run = True\n",
        "if debug_run:\n",
        "  config_default[\"epochs\"] = 1\n",
        "  config_default[\"num_rounds\"] = 1\n",
        "  config_default[\"dataset_size_limit\"] = config_default[\"batch_size\"]*2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMAv7OGM7ZNC"
      },
      "source": [
        "do_sweep = False\n",
        "sweep_id = None\n",
        "api_key = \"2107106c7490857d193f1eb18f9c1223e1121209\"\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"Sweep Centralized 1\",\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "        \"client_lr\": {\n",
        "            \"values\": [1e-2, 1e-3, 1e-4]\n",
        "        },\n",
        "        \"model\": {\n",
        "            \"values\": [\"VGG_16\", \"VGG_Simple\"]\n",
        "        },\n",
        "        \"lr_decay\": {\n",
        "            \"values\": [0, 0.05]\n",
        "        },\n",
        "        \"num_clients\": {\n",
        "            \"values\": [1, 2, 3]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzrISIoRnBaw"
      },
      "source": [
        "## Download And Verify Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9P0JsS0fslM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65eed0a-369e-4937-e7ee-2c073d8f2c01"
      },
      "source": [
        "# Original Dataset on: http://ictcf.biocuckoo.cn\n",
        "#NiCT: http://ictcf.biocuckoo.cn/patient/CT/NiCT.zip\n",
        "#pCT: http://ictcf.biocuckoo.cn/patient/CT/pCT.zip\n",
        "#nCT: http://ictcf.biocuckoo.cn/patient/CT/nCT.zip\n",
        "#nCT(no disease): http://ictcf.biocuckoo.cn/patient/CT/nCT%20(No%20disease).zip\n",
        "\n",
        "![ ! -f pCT.tar ] && wget -qO- https://geile.software/projects/thesis/d7.1/pCT.tar | tar x\n",
        "![ ! -f nCT.tar ] && wget -qO- https://geile.software/projects/thesis/d7.1/nCT.tar | tar x\n",
        "![ ! -f NiCT.tar ] && wget -qO- https://geile.software/projects/thesis/d7.1/NiCT.tar | tar x\n",
        "![ ! -f checksums ] && wget -q https://geile.software/projects/thesis/d7.1/checksums\n",
        "\n",
        "# create reproducible tars: http://h2.jaguarpaw.co.uk/posts/reproducible-tar/\n",
        "![ ! -f pCT.tar ] && tar --sort=name --mtime='2015-10-21 00:00Z' --owner=0 --group=0 --numeric-owner --format=gnu --mode=\"go-rwx,u-rw\" -cf pCT.tar pCT/\n",
        "![ ! -f nCT.tar ] && tar --sort=name --mtime='2015-10-21 00:00Z' --owner=0 --group=0 --numeric-owner --format=gnu --mode=\"go-rwx,u-rw\" -cf nCT.tar nCT/\n",
        "![ ! -f NiCT.tar ] && tar --sort=name --mtime='2015-10-21 00:00Z' --owner=0 --group=0 --numeric-owner --format=gnu --mode=\"go-rwx,u-rw\" -cf NiCT.tar NiCT/\n",
        "\n",
        "import os\n",
        "if os.system('cat checksums | sha256sum --check --status') != 0:\n",
        "  raise Exception(\"Download Checksums wrong\")\n",
        "else:\n",
        "  print(\"Checksums correct\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checksums correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA8OAedZq8Gr"
      },
      "source": [
        "## Dependencies for d7.1\n",
        "To use Inception Net V3 and ChexNet model in your application developments, you must have installed the following dependencies:\n",
        "\n",
        "Python 3.7\n",
        "\n",
        "OpenCV-python 3.4.2\n",
        "\n",
        "Scikit-image 0.15.0\n",
        "\n",
        "Scikit-learn 0.21.2\n",
        "\n",
        "Tensorflow 1.13.1\n",
        "\n",
        "Keras 2.2.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf2p9mF_sctH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb779af-3fee-4bed-82c2-1929df2446ee"
      },
      "source": [
        "![ -f pip-setup-finished ] && echo \"dependencies already set up on this runtime\"\n",
        "![ ! -f pip-setup-finished ] && pip --version\n",
        "![ ! -f pip-setup-finished ] && pip install --upgrade pip\n",
        "![ ! -f pip-setup-finished ] && pip uninstall -y tensorflow tensorflow-gpu opencv-python scikit-image scikit-learn keras\n",
        "![ ! -f pip-setup-finished ] && pip install tensorflow-gpu==1.13.1\n",
        "![ ! -f pip-setup-finished ] && pip install keras==2.2.4\n",
        "![ ! -f pip-setup-finished ] && pip install opencv-python==3.4.2.17\n",
        "![ ! -f pip-setup-finished ] && pip install scikit-image==0.15.0\n",
        "![ ! -f pip-setup-finished ] && pip install scikit-learn==0.22\n",
        "\n",
        "# TFF Additions\n",
        "![ ! -f pip-setup-finished ] && pip install tensorflow_federated==0.18.0\n",
        "![ ! -f pip-setup-finished ] && pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "![ ! -f pip-setup-finished ] && pip install wandb\n",
        "\n",
        "!touch pip-setup-finished\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dependencies already set up on this runtime\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhKV4CqLsSTo"
      },
      "source": [
        "import os\n",
        "import cv2 \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import ChainMap\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import load_model  \n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Softmax,Activation,Dense,Dropout\n",
        "from tensorflow.keras.callbacks import Callback,ModelCheckpoint\n",
        "from sklearn.linear_model import LogisticRegressionCV \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import sklearn\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0kaz6BCo9tB"
      },
      "source": [
        "# Set random seeds\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "np.random.seed(config_default[\"random_seed\"])\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "random.seed(config_default[\"random_seed\"])\n",
        "\n",
        "# The below set_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see:\n",
        "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
        "tf.random.set_seed(config_default[\"random_seed\"])\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(config_default[\"random_seed\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Ea-Vff8xFX"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGLZWMUR8wYm"
      },
      "source": [
        "def corp_margin(img2):\n",
        "    img2=np.asarray(img2)\n",
        "    (row, col) = img2.shape\n",
        "    row_top = 0\n",
        "    raw_down = 0\n",
        "    col_top = 0\n",
        "    col_down = 0\n",
        "    axis1=img2.sum(axis=1)\n",
        "    axis0=img2.sum(axis=0)\n",
        "    for r in range(0, row):\n",
        "        if axis1[r] > 30:\n",
        "            row_top = r\n",
        "            break\n",
        "    for r in range(row - 1, 0, -1):\n",
        "        if axis1[r] > 30:\n",
        "            raw_down = r\n",
        "            break\n",
        "    for c in range(0, col):\n",
        "        if axis0[c] > 30:\n",
        "            col_top = c\n",
        "            break\n",
        "    for c in range(col - 1, 0, -1):\n",
        "        if axis0[c] > 30:\n",
        "            col_down = c\n",
        "            break\n",
        "    a=raw_down+ 1 - row_top-(col_down+ 1-col_top)\n",
        "    if a>0:\n",
        "            w=raw_down+ 1-row_top\n",
        "            col_down=int((col_top+col_down + 1)/2+w/2)\n",
        "            col_top = col_down-w\n",
        "            if col_top < 0:\n",
        "                col_top = 0\n",
        "                col_down = col_top + w\n",
        "            elif col_down >= col:\n",
        "                col_down = col - 1\n",
        "                col_top = col_down - w\n",
        "    else:\n",
        "            w=col_down + 1- col_top\n",
        "            raw_down = int((row_top + raw_down + 1) / 2 + w/2)\n",
        "            row_top =  raw_down-w\n",
        "            if row_top < 0:\n",
        "                row_top = 0\n",
        "                raw_down = row_top + w\n",
        "            elif raw_down >= row:\n",
        "                raw_down = row - 1\n",
        "                row_top = raw_down - w\n",
        "    if row_top==raw_down:\n",
        "        row_top=0\n",
        "        raw_down=99\n",
        "        col_top = 0\n",
        "        col_down = 99\n",
        "    new_img = img2[row_top:raw_down + 1, col_top:col_down + 1]\n",
        "    return new_img\n",
        "\n",
        "\n",
        "def read_ct_img_bydir(target_dir):\n",
        "    img=cv2.imdecode(np.fromfile(target_dir,dtype=np.uint8),cv2.IMREAD_GRAYSCALE)\n",
        "    img = corp_margin(img)\n",
        "    img=cv2.resize(img,(200,200))\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KaCjBss8Wtb"
      },
      "source": [
        "## Model Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6sQhZOhsUa9"
      },
      "source": [
        "def VGG_Simple():\n",
        "    model=Sequential()\n",
        "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(200,200,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',kernel_initializer='uniform',activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Conv2D(16,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(16,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def VGG_16():\n",
        "    input_tensor = Input(shape=(200,200,1))\n",
        "\n",
        "    weight_model = VGG16(weights='imagenet', include_top=False) #Load ResNet50V2 ImageNet pre-trained weights\n",
        "    weight_model.save_weights('weights.h5') #Save the weights\n",
        "    base_model = VGG16(weights=None, include_top=False, input_tensor=input_tensor) #Load the ResNet50V2 model without weights\n",
        "    base_model.load_weights('weights.h5',skip_mismatch=True, by_name=True) #Load the ImageNet weights on the ResNet50V2 model except the first layer(because the first layer has one channel in our case)\n",
        "\n",
        "    model=Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqFga6Rh8cZE"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nbia9fGeFfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d4d462-08f9-4351-8056-7ffb5e6f7519"
      },
      "source": [
        "target_dir1='NiCT/'\n",
        "target_dir2='pCT/'\n",
        "target_dir3='nCT/'\n",
        "target_list1=[target_dir1+file for file in os.listdir(target_dir1)]\n",
        "target_list2=[target_dir2+file for file in os.listdir(target_dir2)]\n",
        "target_list3=[target_dir3+file for file in os.listdir(target_dir3)]\n",
        "\n",
        "if config_default[\"dataset_size_limit\"] != None:\n",
        "  target_list1 = random.sample(target_list1, config_default[\"dataset_size_limit\"])\n",
        "  target_list2 = random.sample(target_list2, config_default[\"dataset_size_limit\"])\n",
        "  target_list3 = random.sample(target_list3, config_default[\"dataset_size_limit\"])\n",
        "\n",
        "target_list=target_list1+target_list2+target_list3\n",
        "print(f\"len(target_list)={len(target_list)}\")\n",
        "y_list=to_categorical(np.concatenate(np.array([[0]*len(target_list1),\n",
        "                                               [1]*len(target_list2),\n",
        "                                               [2]*len(target_list3)])),3)\n",
        "X=np.array([read_ct_img_bydir(file) for file in target_list])[:,:,:,np.newaxis]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_list, test_size=0.1, stratify=y_list, random_state=config_default[\"random_seed\"])\n",
        "\n",
        "print(f\"len(X_train)={len(X_train)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(target_list)=384\n",
            "len(X_train)=345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIK4YQ0CfOx7"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import collections\n",
        "\n",
        "def get_client_indices_stratified_shuffle_deterministic(num_clients, y, random_seed):\n",
        "  labels = list(y)\n",
        "  overall_indices = list(range(len(y)))\n",
        "\n",
        "  picked_ids_for_client = []\n",
        "\n",
        "  for i in range(num_clients-1):\n",
        "    remaining_splits = num_clients - i\n",
        "    split_percentage = 1 / remaining_splits\n",
        "\n",
        "    splitter = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, train_size = split_percentage, random_state=random_seed+i)\n",
        "    split = list(splitter.split(overall_indices, labels))\n",
        "    picked_ids = split[0][0]\n",
        "    \n",
        "    indices_in_overall = [overall_indices[i] for i in picked_ids]\n",
        "    picked_ids_for_client.append(indices_in_overall)\n",
        "\n",
        "    print(f\"starting with: {overall_indices}\")\n",
        "    # remove picked_ids from overall_indices as well as labels\n",
        "    for i in sorted(picked_ids, reverse=True):\n",
        "      del overall_indices[i]\n",
        "      del labels[i]\n",
        "\n",
        "    print(f\"picked indices: {picked_ids}\\nremaining: {overall_indices}\\n\")\n",
        "\n",
        "  picked_ids_for_client.append(overall_indices)\n",
        "  return picked_ids_for_client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr-X307_p16m"
      },
      "source": [
        "# Cache Datasets\n",
        "from functools import lru_cache\n",
        "@lru_cache(maxsize=16)\n",
        "def get_dataset_for_client(client_id, num_clients, random_seed):\n",
        "  ind = get_client_indices_stratified_shuffle_deterministic(num_clients, y_train, random_seed)[client_id]\n",
        "\n",
        "  print(f\"len(X_train)={len(X_train)}\\nLimiting to {len(ind)} samples with indices: {ind}\")\n",
        "  xt = np.array([X_train[i] for i in ind])\n",
        "  yt = np.array([y_train[i] for i in ind])\n",
        "  print(f\"len(xt)={len(xt)}\")\n",
        "\n",
        "  return (xt, yt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GI73ur6ml03"
      },
      "source": [
        "class MetricsEvaluation(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data=()):\n",
        "        super(Callback, self).__init__()\n",
        "        self.x_val,self.y_val = validation_data\n",
        "        self.best_val_loss = float(\"inf\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, log={}):\n",
        "        '''\n",
        "        We gather the following metrics (per class and as avg):\n",
        "        - accuracy\n",
        "        - recall/sensitivity\n",
        "        - specificity\n",
        "        - precision (positive predictive value)\n",
        "        - f1-score\n",
        "\n",
        "        [1] https://onlinelibrary.wiley.com/doi/full/10.1111/j.1651-2227.2006.00180.x\n",
        "        '''\n",
        "        epoch = epoch + 1\n",
        "\n",
        "        predicted_class = self.model.predict_classes(self.x_val)\n",
        "        true_class = np.argmax(self.y_val, axis=1)\n",
        "        label_names = [\"nict\", \"pct\", \"nct\"]\n",
        "        report = compute_metrics(true_class, predicted_class, label_names)\n",
        "        log[\"epoch\"] = epoch\n",
        "        log[\"val_metrics\"] = report\n",
        "        del log[\"val_acc\"]\n",
        "        print(f\"epoch {epoch} log={log}\")\n",
        "        wandb.log(log)\n",
        "\n",
        "        # Summarize if best epoch\n",
        "        if log[\"val_loss\"] < self.best_val_loss:\n",
        "          print(f\"val_loss improved from {self.best_val_loss} to {log['val_loss']} in epoch {epoch}\")\n",
        "          self.best_val_loss = log[\"val_loss\"]\n",
        "\n",
        "          wandb.summary[\"best_epoch\"] = epoch\n",
        "          wandb.summary[\"val_loss\"] = self.best_val_loss\n",
        "          wandb.summary[\"best_val_confusion_matrix\"] = sklearn.metrics.confusion_matrix(true_class, predicted_class)\n",
        "          wandb.summary[\"val_metrics\"] = report\n",
        "\n",
        "          self.best_report = report\n",
        "\n",
        "    def on_federated_round_end(self, round_num, state, metrics):\n",
        "        print(f'round {round_num}, metrics={metrics}')\n",
        "\n",
        "        state.model.assign_weights_to(self.model)\n",
        "        val_loss, val_acc = self.model.evaluate(X_val, y_val, verbose=0)\n",
        "        \n",
        "        # Epoch-1 because we add 1 to the callback in on_epoch_end and count them starting at 1 instead of 0\n",
        "        epoch = (round_num * config.epochs_per_client) - 1\n",
        "        self.on_epoch_end(epoch, log={\"round\": round_num,\n",
        "                                      \"acc\": metrics[\"train\"][\"categorical_accuracy\"],\n",
        "                                      \"loss\": metrics[\"train\"][\"loss\"],\n",
        "                                      \"val_acc\": val_acc,\n",
        "                                      \"val_loss\": val_loss})\n",
        "    def submit_best(self):      \n",
        "      wandb.summary[\"val_loss\"] = self.best_val_loss\n",
        "      wandb.summary[\"val_metrics\"] = self.best_report\n",
        "\n",
        "def acc(confusion_matrix):\n",
        "  tn, fp, fn, tp = confusion_matrix.ravel()\n",
        "  return (tp+tn)/(tn+fp+fn+tp)\n",
        "\n",
        "def per_class_accuracy(ytrue, ypred):\n",
        "  mcm = sklearn.metrics.multilabel_confusion_matrix(ytrue, ypred)\n",
        "  return [acc(cm) for cm in mcm]\n",
        "\n",
        "def sp(confusion_matrix):\n",
        "  tn, fp, fn, tp = confusion_matrix.ravel()\n",
        "  return (tn)/(tn+fp)\n",
        "\n",
        "def per_class_specificity(ytrue, ypred):\n",
        "  mcm = sklearn.metrics.multilabel_confusion_matrix(ytrue, ypred)\n",
        "  return [sp(cm) for cm in mcm]\n",
        "\n",
        "def compute_metrics(true_class, predicted_class, label_names):\n",
        "  report = sklearn.metrics.classification_report(true_class, predicted_class, target_names = label_names, output_dict=True)\n",
        "\n",
        "  accs = per_class_accuracy(true_class, predicted_class)\n",
        "  specs = per_class_specificity(true_class,predicted_class)\n",
        "  for i in range(3):\n",
        "    report[label_names[i]][\"accuracy\"] = accs[i]\n",
        "    report[label_names[i]][\"specificity\"] = specs[i]\n",
        "\n",
        "  print(sklearn.metrics.classification_report(true_class, predicted_class, target_names = label_names))\n",
        "  return report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIJQykYfWZJO"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def train():\n",
        "  K.clear_session()\n",
        "  run = wandb.init(config=config_default)\n",
        "  print(run.config)\n",
        "  run.name=f\"{{model: {run.config.model.lower()}, client_lr: {run.config.client_lr}, lr_decay: {run.config.lr_decay}, num_clients: {run.config.num_clients}}}\"\n",
        "  checkpoint = ModelCheckpoint(os.path.join(wandb.run.dir, \"model.h5\"), save_weights_only = False, monitor='val_loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
        "  AdditionalMetrics = MetricsEvaluation(validation_data=(X_val,y_val))\n",
        "\n",
        "  xt, yt = get_dataset_for_client(run.config.central_dataset_index, run.config.num_clients, run.config.random_seed)\n",
        "\n",
        "  model = None\n",
        "  if run.config.model == \"VGG_16\":\n",
        "      model = VGG_16()\n",
        "  else:\n",
        "      model = VGG_Simple()\n",
        "\n",
        "  model.compile(optimizer=optimizers.Adam(lr=run.config.client_lr, decay=run.config.lr_decay), loss= run.config.loss_function,metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  wandb.summary['params'] = model.count_params()\n",
        "  wandb.summary['len_train'] = len(xt)\n",
        "\n",
        "  metrics_calculator = MetricsEvaluation((X_val, y_val))\n",
        "  metrics_calculator.model = model\n",
        "\n",
        "  # round 0 will be the freshly initialized model with no training\n",
        "  loss, acc = model.evaluate(xt, yt, verbose=0)\n",
        "  val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "  metrics_calculator.on_epoch_end(-1, log={\"acc\": acc,\"loss\": loss,\"val_acc\": val_acc, \"val_loss\": val_loss})\n",
        "\n",
        "  # train other rounds\n",
        "  history = model.fit(xt, yt, epochs=run.config.epochs, batch_size=run.config.batch_size, class_weight = 'auto', validation_data=(X_val, y_val), callbacks=[checkpoint,AdditionalMetrics],verbose=1)\n",
        "  metrics_calculator.submit_best()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZqgeAgc07nOC",
        "outputId": "5a635b17-7158-4604-82ec-016c9c75320c"
      },
      "source": [
        "if do_sweep:\n",
        "  if sweep_id == None:\n",
        "      sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
        "  wandb.agent(sweep_id, function=train, project=project_name)\n",
        "else:\n",
        "  train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.27<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">{model: vgg_16, client_lr: 0.01, lr_decay: 0, num_clients: 3}</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/maxb/Thesis%20d7.1%20%28development%29\" target=\"_blank\">https://wandb.ai/maxb/Thesis%20d7.1%20%28development%29</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/maxb/Thesis%20d7.1%20%28development%29/sweeps/p863eun6\" target=\"_blank\">https://wandb.ai/maxb/Thesis%20d7.1%20%28development%29/sweeps/p863eun6</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/maxb/Thesis%20d7.1%20%28development%29/runs/niomqsmh\" target=\"_blank\">https://wandb.ai/maxb/Thesis%20d7.1%20%28development%29/runs/niomqsmh</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210420_005654-niomqsmh</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'client_lr': 0.01, 'lr_decay': 0, 'model': 'VGG_16', 'num_clients': 3, 'central_dataset_index': 0, 'epochs': 1, 'batch_size': 64, 'loss_function': 'categorical_crossentropy', 'dataset': 'HUST-19 (slices)', 'dataset_size_limit': 128, 'sampling': None, 'random_seed': 4242, 'num_rounds': 1}\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "starting with: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344]\n",
            "picked indices: [158  72 334 294 136  14 235  49 335 163 237 336  30 164 209 184 190  77\n",
            "  79 282   7 115 229 281 198 300 214 147  83 215  90 162 276 322  32 293\n",
            " 218 302  94 101 337 168  84 245  70 150  76 242 111   9 323  73 234  71\n",
            " 212  60 102 113 254 192 272 220 268  23 305 340  13  97 120 118 154 126\n",
            " 108 257 216  19  98  61  40  39  88 133 315 310 232  20 343  46  62   3\n",
            " 298  99 175 303  27 255 128 112 187  43  67 110 331  96  38 144  52  63\n",
            " 210 182 202 299 279 313 228]\n",
            "remaining: [0, 1, 2, 4, 5, 6, 8, 10, 11, 12, 15, 16, 17, 18, 21, 22, 24, 25, 26, 28, 29, 31, 33, 34, 35, 36, 37, 41, 42, 44, 45, 47, 48, 50, 51, 53, 54, 55, 56, 57, 58, 59, 64, 65, 66, 68, 69, 74, 75, 78, 80, 81, 82, 85, 86, 87, 89, 91, 92, 93, 95, 100, 103, 104, 105, 106, 107, 109, 114, 116, 117, 119, 121, 122, 123, 124, 125, 127, 129, 130, 131, 132, 134, 135, 137, 138, 139, 140, 141, 142, 143, 145, 146, 148, 149, 151, 152, 153, 155, 156, 157, 159, 160, 161, 165, 166, 167, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 183, 185, 186, 188, 189, 191, 193, 194, 195, 196, 197, 199, 200, 201, 203, 204, 205, 206, 207, 208, 211, 213, 217, 219, 221, 222, 223, 224, 225, 226, 227, 230, 231, 233, 236, 238, 239, 240, 241, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270, 271, 273, 274, 275, 277, 278, 280, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 295, 296, 297, 301, 304, 306, 307, 308, 309, 311, 312, 314, 316, 317, 318, 319, 320, 321, 324, 325, 326, 327, 328, 329, 330, 332, 333, 338, 339, 341, 342, 344]\n",
            "\n",
            "starting with: [0, 1, 2, 4, 5, 6, 8, 10, 11, 12, 15, 16, 17, 18, 21, 22, 24, 25, 26, 28, 29, 31, 33, 34, 35, 36, 37, 41, 42, 44, 45, 47, 48, 50, 51, 53, 54, 55, 56, 57, 58, 59, 64, 65, 66, 68, 69, 74, 75, 78, 80, 81, 82, 85, 86, 87, 89, 91, 92, 93, 95, 100, 103, 104, 105, 106, 107, 109, 114, 116, 117, 119, 121, 122, 123, 124, 125, 127, 129, 130, 131, 132, 134, 135, 137, 138, 139, 140, 141, 142, 143, 145, 146, 148, 149, 151, 152, 153, 155, 156, 157, 159, 160, 161, 165, 166, 167, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 183, 185, 186, 188, 189, 191, 193, 194, 195, 196, 197, 199, 200, 201, 203, 204, 205, 206, 207, 208, 211, 213, 217, 219, 221, 222, 223, 224, 225, 226, 227, 230, 231, 233, 236, 238, 239, 240, 241, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270, 271, 273, 274, 275, 277, 278, 280, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 295, 296, 297, 301, 304, 306, 307, 308, 309, 311, 312, 314, 316, 317, 318, 319, 320, 321, 324, 325, 326, 327, 328, 329, 330, 332, 333, 338, 339, 341, 342, 344]\n",
            "picked indices: [ 33 184  10  11 118  76 196 144 162  90 134   4 228  36  54 214 193 117\n",
            "  12  65  22  89  64 216  35 179 129 211  47  80 154 221   8 131  61  72\n",
            "  88  53 183 158 103  17 125 120 197 202 194 152 148 167 200 164  96   2\n",
            "  16  19 210  34 135 192 190  69  91 124  59 146 119 199  46 165 128  85\n",
            "  43 223  13  62 100  55  84 145 140 104  21   6  26 116  66 218 163  56\n",
            " 160 143 112 220 141 155 226  82 201 121 188 206  37 171  73 217 207  78\n",
            "  44  41  30 161 208  58  49]\n",
            "remaining: [0, 1, 4, 6, 10, 12, 21, 22, 26, 29, 34, 35, 36, 41, 42, 44, 47, 48, 56, 57, 58, 64, 68, 75, 80, 81, 82, 91, 95, 104, 109, 114, 117, 119, 123, 124, 127, 130, 132, 135, 139, 140, 146, 148, 149, 151, 153, 155, 156, 159, 160, 166, 167, 169, 170, 171, 172, 173, 176, 177, 178, 188, 189, 194, 195, 199, 201, 203, 206, 207, 208, 211, 219, 225, 227, 230, 231, 236, 240, 241, 244, 252, 256, 258, 259, 261, 262, 263, 264, 265, 266, 267, 270, 271, 273, 277, 278, 280, 284, 286, 290, 295, 306, 307, 308, 314, 318, 319, 321, 327, 330, 333, 338, 341, 344]\n",
            "\n",
            "len(X_train)=345\n",
            "Limiting to 115 samples with indices: [158, 72, 334, 294, 136, 14, 235, 49, 335, 163, 237, 336, 30, 164, 209, 184, 190, 77, 79, 282, 7, 115, 229, 281, 198, 300, 214, 147, 83, 215, 90, 162, 276, 322, 32, 293, 218, 302, 94, 101, 337, 168, 84, 245, 70, 150, 76, 242, 111, 9, 323, 73, 234, 71, 212, 60, 102, 113, 254, 192, 272, 220, 268, 23, 305, 340, 13, 97, 120, 118, 154, 126, 108, 257, 216, 19, 98, 61, 40, 39, 88, 133, 315, 310, 232, 20, 343, 46, 62, 3, 298, 99, 175, 303, 27, 255, 128, 112, 187, 43, 67, 110, 331, 96, 38, 144, 52, 63, 210, 182, 202, 299, 279, 313, 228]\n",
            "len(xt)=115\n",
            "WARNING:tensorflow:Skipping loading of weights for layer block1_conv1 due to mismatch in shape ((3, 3, 1, 64) vs (64, 3, 3, 3)).\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 6, 6, 512)         14713536  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 55299     \n",
            "=================================================================\n",
            "Total params: 14,768,835\n",
            "Trainable params: 14,768,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4c78c12680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        nict       0.14      0.08      0.10        13\n",
            "         pct       0.35      0.85      0.50        13\n",
            "         nct       0.00      0.00      0.00        13\n",
            "\n",
            "    accuracy                           0.31        39\n",
            "   macro avg       0.17      0.31      0.20        39\n",
            "weighted avg       0.17      0.31      0.20        39\n",
            "\n",
            "epoch 0 log={'acc': 0.2956521809101105, 'loss': 1.4148017168045044, 'val_loss': 1.302288293838501, 'epoch': 0, 'val_metrics': {'nict': {'precision': 0.14285714285714285, 'recall': 0.07692307692307693, 'f1-score': 0.1, 'support': 13, 'accuracy': 0.5384615384615384, 'specificity': 0.7692307692307693}, 'pct': {'precision': 0.3548387096774194, 'recall': 0.8461538461538461, 'f1-score': 0.5, 'support': 13, 'accuracy': 0.4358974358974359, 'specificity': 0.23076923076923078}, 'nct': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 13, 'accuracy': 0.6410256410256411, 'specificity': 0.9615384615384616}, 'accuracy': 0.3076923076923077, 'macro avg': {'precision': 0.16589861751152074, 'recall': 0.3076923076923077, 'f1-score': 0.19999999999999998, 'support': 39}, 'weighted avg': {'precision': 0.16589861751152074, 'recall': 0.3076923076923077, 'f1-score': 0.19999999999999998, 'support': 39}}}\n",
            "val_loss improved from inf to 1.302288293838501 in epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-55b39a13e8bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-0e59b886924b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# train other rounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdditionalMetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mmetrics_calculator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_make_class_weight_map_fn\u001b[0;34m(class_weight)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0mweighting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m   \"\"\"\n\u001b[0;32m-> 1278\u001b[0;31m   \u001b[0mclass_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m   \u001b[0mexpected_class_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_class_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
          ]
        }
      ]
    }
  ]
}