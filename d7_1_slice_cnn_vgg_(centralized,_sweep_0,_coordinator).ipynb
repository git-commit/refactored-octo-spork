{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "d7.1 slice cnn vgg (centralized, sweep 0, coordinator)",
      "provenance": [],
      "collapsed_sections": [
        "29siXEYv9fNS",
        "G9Ea-Vff8xFX"
      ],
      "authorship_tag": "ABX9TyPG8QSIk4v+xqpqEH52IhDn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-commit/refactored-octo-spork/blob/main/d7_1_slice_cnn_vgg_(centralized%2C_sweep_0%2C_coordinator).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihL-Q_WeycsB"
      },
      "source": [
        "# Centralized Learning experiment with D7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ypjjMiG-Kw3"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEYNQKVU370g"
      },
      "source": [
        "project_name = \"Thesis d7.1 (production)\"\n",
        "num_clients = 1\n",
        "\n",
        "config = {\"num_clients\": num_clients,\n",
        "        \"client_lr\":0.0001,\n",
        "        \"lr_decay\": 0.05,\n",
        "        \"epochs\": 150,\n",
        "        \"batch_size\":64,\n",
        "        \"loss_function\": \"categorical_crossentropy\",\n",
        "        \"model\": \"VGG_Simple\",\n",
        "        \"dataset\": \"HUST-19 (slices)\",\n",
        "        \"dataset_size_limit\": None,\n",
        "        \"sampling\": None if num_clients < 2 else \"stratified_shuffle_deterministic\",\n",
        "        \"random_seed\": 4242\n",
        "      }\n",
        "\n",
        "debug_run = False\n",
        "if debug_run:\n",
        "  config[\"epochs\"] = 1\n",
        "  config[\"num_rounds\"] = 1\n",
        "  config[\"dataset_size_limit\"] = config[\"batch_size\"]*2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29siXEYv9fNS"
      },
      "source": [
        "## Download And Verify Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9P0JsS0fslM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "b607b1df-253e-4ea6-f008-a132645a0959"
      },
      "source": [
        "# Original Dataset on: http://ictcf.biocuckoo.cn\n",
        "#NiCT: http://ictcf.biocuckoo.cn/patient/CT/NiCT.zip\n",
        "#pCT: http://ictcf.biocuckoo.cn/patient/CT/pCT.zip\n",
        "#nCT: http://ictcf.biocuckoo.cn/patient/CT/nCT.zip\n",
        "#nCT(no disease): http://ictcf.biocuckoo.cn/patient/CT/nCT%20(No%20disease).zip\n",
        "\n",
        "![ ! -f pCT.tar ] && wget -qO- https://geile.software/projects/thesis/d7.1/pCT.tar | tar x\n",
        "![ ! -f nCT.tar ] && wget -qO- https://geile.software/projects/thesis/d7.1/nCT.tar | tar x\n",
        "![ ! -f NiCT.tar ] && wget -qO- https://geile.software/projects/thesis/d7.1/NiCT.tar | tar x\n",
        "![ ! -f checksums ] && wget -q https://geile.software/projects/thesis/d7.1/checksums\n",
        "\n",
        "# create reproducible tars: http://h2.jaguarpaw.co.uk/posts/reproducible-tar/\n",
        "![ ! -f pCT.tar ] && tar --sort=name --mtime='2015-10-21 00:00Z' --owner=0 --group=0 --numeric-owner --format=gnu --mode=\"go-rwx,u-rw\" -cf pCT.tar pCT/\n",
        "![ ! -f nCT.tar ] && tar --sort=name --mtime='2015-10-21 00:00Z' --owner=0 --group=0 --numeric-owner --format=gnu --mode=\"go-rwx,u-rw\" -cf nCT.tar nCT/\n",
        "![ ! -f NiCT.tar ] && tar --sort=name --mtime='2015-10-21 00:00Z' --owner=0 --group=0 --numeric-owner --format=gnu --mode=\"go-rwx,u-rw\" -cf NiCT.tar NiCT/\n",
        "\n",
        "import os\n",
        "if os.system('cat checksums | sha256sum --check --status') != 0:\n",
        "  raise Exception(\"Download Checksums wrong\")\n",
        "else:\n",
        "  print(\"Checksums correct\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7a1387cbdedc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat checksums | sha256sum --check --status'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Download Checksums wrong\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checksums correct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Download Checksums wrong"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA8OAedZq8Gr"
      },
      "source": [
        "## Dependencies for d7.1\n",
        "To use Inception Net V3 and ChexNet model in your application developments, you must have installed the following dependencies:\n",
        "\n",
        "Python 3.7\n",
        "\n",
        "OpenCV-python 3.4.2\n",
        "\n",
        "Scikit-image 0.15.0\n",
        "\n",
        "Scikit-learn 0.21.2\n",
        "\n",
        "Tensorflow 1.13.1\n",
        "\n",
        "Keras 2.2.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf2p9mF_sctH"
      },
      "source": [
        "![ -f pip-setup-finished ] && echo \"dependencies already set up on this runtime\"\n",
        "![ ! -f pip-setup-finished ] && pip --version\n",
        "![ ! -f pip-setup-finished ] && pip uninstall -y tensorflow tensorflow-gpu opencv-python scikit-image scikit-learn keras\n",
        "![ ! -f pip-setup-finished ] && pip install tensorflow-gpu==1.13.1\n",
        "![ ! -f pip-setup-finished ] && pip install keras==2.2.4\n",
        "![ ! -f pip-setup-finished ] && pip install opencv-python==3.4.2.17\n",
        "![ ! -f pip-setup-finished ] && pip install scikit-image==0.15.0\n",
        "![ ! -f pip-setup-finished ] && pip install scikit-learn==0.22\n",
        "\n",
        "![ ! -f pip-setup-finished ] && pip install wandb\n",
        "\n",
        "!touch pip-setup-finished\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVB83x47xnsP"
      },
      "source": [
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhKV4CqLsSTo"
      },
      "source": [
        "import os\n",
        "import cv2 \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import ChainMap\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import load_model  \n",
        "from keras.utils import plot_model\n",
        "from keras import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,Softmax,Activation,Dense,Dropout\n",
        "from keras.callbacks import Callback,ModelCheckpoint\n",
        "from sklearn.linear_model import LogisticRegressionCV \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from keras import optimizers\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Input\n",
        "import keras\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0kaz6BCo9tB"
      },
      "source": [
        "# Set random seeds\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "np.random.seed(config[\"random_seed\"])\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "random.seed(config[\"random_seed\"])\n",
        "\n",
        "# The below set_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see:\n",
        "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
        "tf.random.set_random_seed(config[\"random_seed\"])\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(config[\"random_seed\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Ea-Vff8xFX"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGLZWMUR8wYm"
      },
      "source": [
        "def corp_margin(img2):\n",
        "    img2=np.asarray(img2)\n",
        "    (row, col) = img2.shape\n",
        "    row_top = 0\n",
        "    raw_down = 0\n",
        "    col_top = 0\n",
        "    col_down = 0\n",
        "    axis1=img2.sum(axis=1)\n",
        "    axis0=img2.sum(axis=0)\n",
        "    for r in range(0, row):\n",
        "        if axis1[r] > 30:\n",
        "            row_top = r\n",
        "            break\n",
        "    for r in range(row - 1, 0, -1):\n",
        "        if axis1[r] > 30:\n",
        "            raw_down = r\n",
        "            break\n",
        "    for c in range(0, col):\n",
        "        if axis0[c] > 30:\n",
        "            col_top = c\n",
        "            break\n",
        "    for c in range(col - 1, 0, -1):\n",
        "        if axis0[c] > 30:\n",
        "            col_down = c\n",
        "            break\n",
        "    a=raw_down+ 1 - row_top-(col_down+ 1-col_top)\n",
        "    if a>0:\n",
        "            w=raw_down+ 1-row_top\n",
        "            col_down=int((col_top+col_down + 1)/2+w/2)\n",
        "            col_top = col_down-w\n",
        "            if col_top < 0:\n",
        "                col_top = 0\n",
        "                col_down = col_top + w\n",
        "            elif col_down >= col:\n",
        "                col_down = col - 1\n",
        "                col_top = col_down - w\n",
        "    else:\n",
        "            w=col_down + 1- col_top\n",
        "            raw_down = int((row_top + raw_down + 1) / 2 + w/2)\n",
        "            row_top =  raw_down-w\n",
        "            if row_top < 0:\n",
        "                row_top = 0\n",
        "                raw_down = row_top + w\n",
        "            elif raw_down >= row:\n",
        "                raw_down = row - 1\n",
        "                row_top = raw_down - w\n",
        "    if row_top==raw_down:\n",
        "        row_top=0\n",
        "        raw_down=99\n",
        "        col_top = 0\n",
        "        col_down = 99\n",
        "    new_img = img2[row_top:raw_down + 1, col_top:col_down + 1]\n",
        "    return new_img\n",
        "\n",
        "\n",
        "def read_ct_img_bydir(target_dir):\n",
        "    img=cv2.imdecode(np.fromfile(target_dir,dtype=np.uint8),cv2.IMREAD_GRAYSCALE)\n",
        "    img = corp_margin(img)\n",
        "    img=cv2.resize(img,(200,200))\n",
        "    return img\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback,ModelCheckpoint\n",
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=()):\n",
        "        super(Callback, self).__init__()\n",
        "        self.x_val,self.y_val = validation_data\n",
        "    def on_epoch_end(self, epoch, log={}):\n",
        "        y_pred = self.model.predict(self.x_val)\n",
        "        AUC1 = roc_auc_score(self.y_val[:,0], y_pred[:,0])\n",
        "        AUC2 = roc_auc_score(self.y_val[:,1], y_pred[:,1])\n",
        "        AUC3 = roc_auc_score(self.y_val[:,2], y_pred[:,2])\n",
        "        print('val_AUC NiCT epoch:%d: %.6f' % (epoch+1, AUC1))\n",
        "        print('val_AUC pCT epoch:%d: %.6f' % (epoch+1, AUC2))\n",
        "        print('val_AUC nCT epoch:%d: %.6f' % (epoch+1, AUC3))\n",
        "        print(f'val_AUC_avg={(AUC1+AUC2+AUC3)/3}')\n",
        "\n",
        "        wandb.log({\"val_auc_nict\": AUC1,\n",
        "                   \"val_auc_pct\": AUC2,\n",
        "                   \"val_auc_nct\": AUC3}, commit=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KaCjBss8Wtb"
      },
      "source": [
        "## Model Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6sQhZOhsUa9"
      },
      "source": [
        "def VGG_Simple():\n",
        "    model=Sequential()\n",
        "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(200,200,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',kernel_initializer='uniform',activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Conv2D(16,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(16,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def VGG_16():\n",
        "    input_tensor = Input(shape=(200,200,1))\n",
        "\n",
        "    weight_model = VGG16(weights='imagenet', include_top=False) #Load ResNet50V2 ImageNet pre-trained weights\n",
        "    weight_model.save_weights('weights.h5') #Save the weights\n",
        "    base_model = VGG16(weights=None, include_top=False, input_tensor=input_tensor) #Load the ResNet50V2 model without weights\n",
        "    base_model.load_weights('weights.h5',skip_mismatch=True, by_name=True) #Load the ImageNet weights on the ResNet50V2 model except the first layer(because the first layer has one channel in our case)\n",
        "\n",
        "    model=Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqFga6Rh8cZE"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nbia9fGeFfV"
      },
      "source": [
        "target_dir1='NiCT/'\n",
        "target_dir2='pCT/'\n",
        "target_dir3='nCT/'\n",
        "target_list1=[target_dir1+file for file in os.listdir(target_dir1)]\n",
        "target_list2=[target_dir2+file for file in os.listdir(target_dir2)]\n",
        "target_list3=[target_dir3+file for file in os.listdir(target_dir3)]\n",
        "\n",
        "if config[\"dataset_size_limit\"] != None:\n",
        "  target_list1 = random.sample(target_list1, config[\"dataset_size_limit\"])\n",
        "  target_list2 = random.sample(target_list2, config[\"dataset_size_limit\"])\n",
        "  target_list3 = random.sample(target_list3, config[\"dataset_size_limit\"])\n",
        "\n",
        "target_list=target_list1+target_list2+target_list3\n",
        "print(f\"len(target_list)={len(target_list)}\")\n",
        "y_list=to_categorical(np.concatenate(np.array([[0]*len(target_list1),\n",
        "                                               [1]*len(target_list2),\n",
        "                                               [2]*len(target_list3)])),3)\n",
        "X=np.array([read_ct_img_bydir(file) for file in target_list])[:,:,:,np.newaxis]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_list, test_size=0.1, stratify=y_list, random_state=config[\"random_seed\"])\n",
        "\n",
        "print(f\"len(X_train)={len(X_train)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIK4YQ0CfOx7"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import collections\n",
        "\n",
        "def get_two_client_indices_stratified_shuffle_deterministic(y, random_seed):\n",
        "  splitter = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, train_size = 0.5, random_state=random_seed)\n",
        "  x = np.zeros(len(y))\n",
        "  split = list(splitter.split(x,y))\n",
        "  return split[0]\n",
        "\n",
        "def get_client_indices_stratified_shuffle_deterministic(num_clients, y, random_seed):\n",
        "  labels = list(y)\n",
        "  overall_indices = list(range(len(y)))\n",
        "\n",
        "  picked_ids_for_client = []\n",
        "\n",
        "  for i in range(num_clients-1):\n",
        "    remaining_splits = num_clients - i\n",
        "    split_percentage = 1 / remaining_splits\n",
        "\n",
        "    splitter = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, train_size = split_percentage, random_state=random_seed+i)\n",
        "    split = list(splitter.split(overall_indices, labels))\n",
        "    picked_ids = split[0][0]\n",
        "    \n",
        "    indices_in_overall = [overall_indices[i] for i in picked_ids]\n",
        "    picked_ids_for_client.append(indices_in_overall)\n",
        "\n",
        "    print(f\"starting with: {overall_indices}\")\n",
        "    # remove picked_ids from overall_indices as well as labels\n",
        "    for i in sorted(picked_ids, reverse=True):\n",
        "      del overall_indices[i]\n",
        "      del labels[i]\n",
        "\n",
        "    print(f\"picked indices: {picked_ids}\\nremaining: {overall_indices}\\n\")\n",
        "\n",
        "  picked_ids_for_client.append(overall_indices)\n",
        "  return picked_ids_for_client\n",
        "\n",
        "if config[\"num_clients\"] > 1:\n",
        "  ind = get_client_indices_stratified_shuffle_deterministic(y_train, config[\"random_seed\"])[0]\n",
        "  config[\"dataset_size_limit\"] = len(ind)\n",
        "\n",
        "  print(f\"len(X_train)={len(X_train)}\\nLimiting to {config['dataset_size_limit']} samples with indices: {ind}\")\n",
        "  X_train = np.array([X_train[i] for i in ind])\n",
        "  y_train = np.array([y_train[i] for i in ind])\n",
        "  print(f\"len(X_train)={len(X_train)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GI73ur6ml03"
      },
      "source": [
        "class MetricsEvaluation(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data=()):\n",
        "        super(Callback, self).__init__()\n",
        "        self.x_val,self.y_val = validation_data\n",
        "        self.best_val_loss = float(\"inf\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, log={}):\n",
        "        '''\n",
        "        We gather the following metrics (per class and as avg):\n",
        "        - accuracy\n",
        "        - recall/sensitivity\n",
        "        - specificity\n",
        "        - precision (positive predictive value)\n",
        "        - f1-score\n",
        "\n",
        "        [1] https://onlinelibrary.wiley.com/doi/full/10.1111/j.1651-2227.2006.00180.x\n",
        "        '''\n",
        "        epoch = epoch + 1\n",
        "\n",
        "        predicted_class = self.model.predict_classes(self.x_val)\n",
        "        true_class = np.argmax(self.y_val, axis=1)\n",
        "        label_names = [\"nict\", \"pct\", \"nct\"]\n",
        "        report = compute_metrics(true_class, predicted_class, label_names)\n",
        "        log[\"epoch\"] = epoch\n",
        "        log[\"val_metrics\"] = report\n",
        "        del log[\"val_acc\"]\n",
        "        print(f\"epoch {epoch} log={log}\")\n",
        "        wandb.log(log)\n",
        "\n",
        "        # Summarize if best epoch\n",
        "        if log[\"val_loss\"] < self.best_val_loss:\n",
        "          print(f\"val_loss improved from {self.best_val_loss} to {log['val_loss']} in epoch {epoch}\")\n",
        "          self.best_val_loss = log[\"val_loss\"]\n",
        "\n",
        "          wandb.summary[\"best_epoch\"] = epoch\n",
        "          wandb.summary[\"val_loss\"] = self.best_val_loss\n",
        "          wandb.summary[\"best_val_confusion_matrix\"] = sklearn.metrics.confusion_matrix(true_class, predicted_class)\n",
        "          wandb.summary[\"val_metrics\"] = report\n",
        "\n",
        "          self.best_report = report\n",
        "\n",
        "    def on_federated_round_end(self, round_num, state, metrics):\n",
        "        print(f'round {round_num}, metrics={metrics}')\n",
        "\n",
        "        state.model.assign_weights_to(self.model)\n",
        "        val_loss, val_acc = self.model.evaluate(X_val, y_val, verbose=0)\n",
        "        \n",
        "        # Epoch-1 because we add 1 to the callback in on_epoch_end and count them starting at 1 instead of 0\n",
        "        epoch = (round_num * config.epochs_per_client) - 1\n",
        "        self.on_epoch_end(epoch, log={\"round\": round_num,\n",
        "                                      \"acc\": metrics[\"train\"][\"categorical_accuracy\"],\n",
        "                                      \"loss\": metrics[\"train\"][\"loss\"],\n",
        "                                      \"val_acc\": val_acc,\n",
        "                                      \"val_loss\": val_loss})\n",
        "    def submit_best(self):      \n",
        "      wandb.summary[\"val_loss\"] = self.best_val_loss\n",
        "      wandb.summary[\"val_metrics\"] = self.best_report\n",
        "\n",
        "def acc(confusion_matrix):\n",
        "  tn, fp, fn, tp = confusion_matrix.ravel()\n",
        "  return (tp+tn)/(tn+fp+fn+tp)\n",
        "\n",
        "def per_class_accuracy(ytrue, ypred):\n",
        "  mcm = sklearn.metrics.multilabel_confusion_matrix(ytrue, ypred)\n",
        "  return [acc(cm) for cm in mcm]\n",
        "\n",
        "def sp(confusion_matrix):\n",
        "  tn, fp, fn, tp = confusion_matrix.ravel()\n",
        "  return (tn)/(tn+fp)\n",
        "\n",
        "def per_class_specificity(ytrue, ypred):\n",
        "  mcm = sklearn.metrics.multilabel_confusion_matrix(ytrue, ypred)\n",
        "  return [sp(cm) for cm in mcm]\n",
        "\n",
        "def compute_metrics(true_class, predicted_class, label_names):\n",
        "  report = sklearn.metrics.classification_report(true_class, predicted_class, target_names = label_names, output_dict=True)\n",
        "\n",
        "  accs = per_class_accuracy(true_class, predicted_class)\n",
        "  specs = per_class_specificity(true_class,predicted_class)\n",
        "  for i in range(3):\n",
        "    report[label_names[i]][\"accuracy\"] = accs[i]\n",
        "    report[label_names[i]][\"specificity\"] = specs[i]\n",
        "\n",
        "  print(sklearn.metrics.classification_report(true_class, predicted_class, target_names = label_names))\n",
        "  return report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIJQykYfWZJO"
      },
      "source": [
        "from keras import backend as K\n",
        "def train():\n",
        "  K.clear_session()\n",
        "  run = wandb.init(config=config)\n",
        "  print(run.config)\n",
        "  run.name=f\"{{model: \\\"{run.config.model.lower()}\\\", client_lr: {run.config.client_lr}, lr_decay: {run.config.lr_decay}, num_clients: {run.config.num_clients}}}\"\n",
        "  checkpoint = ModelCheckpoint(os.path.join(wandb.run.dir, \"model.h5\"), save_weights_only = False, monitor='val_loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
        "  RocAuc = RocAucEvaluation(validation_data=(X_val,y_val))\n",
        "  AdditionalMetrics = MetricsEvaluation(validation_data=(X_val,y_val))\n",
        "\n",
        "  model = None\n",
        "  if run.config.model == \"VGG_16\":\n",
        "      model = VGG_16()\n",
        "  else:\n",
        "      model = VGG_Simple()\n",
        "\n",
        "  model.compile(optimizer=optimizers.Adam(lr=run.config.client_lr, decay=run.config.lr_decay), loss= run.config.loss_function,metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  wandb.summary['params'] = model.count_params()\n",
        "  wandb.summary['len_train'] = len(X_train)\n",
        "\n",
        "  metrics_calculator = MetricsEvaluation((X_val, y_val))\n",
        "  metrics_calculator.model = model\n",
        "\n",
        "  # round 0 will be the freshly initialized model with no training\n",
        "  loss, acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "  val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "  metrics_calculator.on_epoch_end(-1, log={\"acc\": acc,\"loss\": loss,\"val_acc\": val_acc, \"val_loss\": val_loss})\n",
        "\n",
        "  # train other rounds\n",
        "  history = model.fit(X_train, y_train, epochs=run.config.epochs, batch_size=run.config.batch_size, class_weight = 'auto', validation_data=(X_val, y_val), callbacks=[checkpoint,RocAuc,AdditionalMetrics],verbose=1)\n",
        "  metrics_calculator.submit_best()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqgeAgc07nOC"
      },
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"Sweep Centralized 1\",\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "        \"client_lr\": {\n",
        "            \"values\": [1e-2, 1e-3, 1e-4]\n",
        "        },\n",
        "        \"model\": {\n",
        "            \"values\": [\"VGG_16\", \"VGG_Simple\"]\n",
        "        },\n",
        "        \"lr_decay\": {\n",
        "            \"values\": [0, 0.05]\n",
        "        },\n",
        "        \"num_clients\": {\n",
        "            \"values\": [3]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
        "wandb.agent(sweep_id, function=train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}